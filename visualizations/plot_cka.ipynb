{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MUJOCO_GL=glfw\n",
      "env: PYOPENGL_PLATFORM=\n",
      "env: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
      "Checking that the dm_control installation succeeded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 19:59:10.911 Python[59119:4315124] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/jm/y1xn0yjj22v8l8rwlryfjdj00000gn/T/org.python.python.savedState\n",
      "/opt/homebrew/lib/python3.10/site-packages/matplotlib/__init__.py:332: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  version = LooseVersion(match.group(1))\n",
      "/opt/homebrew/lib/python3.10/site-packages/gym/envs/registration.py:250: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for plugin in metadata.entry_points().get(entry_point, []):\n",
      "/opt/homebrew/lib/python3.10/site-packages/matplotlib_inline/config.py:68: DeprecationWarning: InlineBackend._figure_format_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n",
      "  def _figure_format_changed(self, name, old, new):\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../src')\n",
    "\n",
    "#Run to install MuJoCo and `dm_control`\n",
    "import distutils.util\n",
    "import subprocess\n",
    "\n",
    "# Use egl locally\n",
    "%env MUJOCO_GL=glfw\n",
    "# Use osmesa on DSMLP\n",
    "# %env MUJOCO_GL=osmesa\n",
    "%env PYOPENGL_PLATFORM=\n",
    "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "\n",
    "print('Checking that the dm_control installation succeeded...')\n",
    "try:\n",
    "    from dm_control import suite\n",
    "    env = suite.load('cartpole', 'swingup')\n",
    "    pixels = env.physics.render()\n",
    "except Exception as e:\n",
    "    raise e from RuntimeError(\n",
    "      'Something went wrong during installation. Check the shell output above '\n",
    "      'for more information.\\n'\n",
    "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "else:\n",
    "    del pixels, suite\n",
    "\n",
    "\n",
    "#All `dm_control` imports required for this tutorial\n",
    "\n",
    "# The basic mujoco wrapper.\n",
    "from dm_control import mujoco\n",
    "\n",
    "# Access to enums and MuJoCo library functions.\n",
    "from dm_control.mujoco.wrapper.mjbindings import enums\n",
    "from dm_control.mujoco.wrapper.mjbindings import mjlib\n",
    "\n",
    "# PyMJCF\n",
    "from dm_control import mjcf\n",
    "\n",
    "# Composer high level imports\n",
    "from dm_control import composer\n",
    "from dm_control.composer.observation import observable\n",
    "from dm_control.composer import variation\n",
    "\n",
    "# Imports for Composer tutorial example\n",
    "from dm_control.composer.variation import distributions\n",
    "from dm_control.composer.variation import noises\n",
    "from dm_control.locomotion.arenas import floors\n",
    "\n",
    "# Control Suite\n",
    "from dm_control import suite\n",
    "\n",
    "# Run through corridor example\n",
    "from dm_control.locomotion.walkers import cmu_humanoid\n",
    "from dm_control.locomotion.arenas import corridors as corridor_arenas\n",
    "from dm_control.locomotion.tasks import corridors as corridor_tasks\n",
    "\n",
    "# # Soccer\n",
    "# from dm_control.locomotion import soccer\n",
    "\n",
    "# Manipulation\n",
    "from dm_control import manipulation\n",
    "\n",
    "#@title Other imports and helper functions\n",
    "\n",
    "# General\n",
    "import copy\n",
    "import os\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "# Graphics-related\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import PIL.Image\n",
    "# Internal loading of video libraries.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.optim import Adam\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# try out the wrappers\n",
    "from acme import wrappers\n",
    "from dm_control import suite\n",
    "from acme import wrappers\n",
    "from model import *\n",
    "from utils import *\n",
    "from analysis import *\n",
    "# Soft-Actor-Critic Model\n",
    "from sac import *\n",
    "from replay_memory import *\n",
    "import argparse\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# Use svg backend for figure rendering\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Font sizes\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# Inline video helper function\n",
    "if os.environ.get('COLAB_NOTEBOOK_TEST', False):\n",
    "  # We skip video generation during tests, as it is quite expensive.\n",
    "  display_video = lambda *args, **kwargs: None\n",
    "else:\n",
    "  def display_video(frames, framerate=30):\n",
    "    height, width, _ = frames[0].shape\n",
    "    dpi = 70\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "      im.set_data(frame)\n",
    "      return [im]\n",
    "    interval = 1000/framerate\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                   interval=interval, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "# Seed numpy's global RNG so that cell outputs are deterministic. We also try to\n",
    "# use RandomState instances that are local to a single cell wherever possible.\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "######  Environment wrappers  ####\n",
    "from dm_env import specs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#@title Loading and simulating a `suite` task{vertical-output: true}\n",
    "\n",
    "# Load the environment\n",
    "# random_state = np.random.RandomState(42)\n",
    "# env = suite.load('hopper', 'stand', task_kwargs={'random': random_state})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from ../data/models/sac_checkpoint_walker_walk_batch512_hidden1024_1123_500\n",
      "linear1 Linear(in_features=24, out_features=1024, bias=True)\n",
      "linear2 Linear(in_features=1024, out_features=1024, bias=True)\n",
      "mean_linear Linear(in_features=1024, out_features=6, bias=True)\n",
      "log_std_linear Linear(in_features=1024, out_features=6, bias=True)\n",
      "Episode: 0 Reward: 887.2914056692971\n",
      "Episode: 1 Reward: 959.9205621872097\n",
      "Episode: 2 Reward: 972.9571659713984\n",
      "Episode: 3 Reward: 951.9114729100838\n",
      "Episode: 4 Reward: 938.7138728490099\n",
      "Episode: 5 Reward: 975.6293765306473\n",
      "Episode: 6 Reward: 937.6452852552757\n",
      "Episode: 7 Reward: 969.0426700860262\n",
      "Episode: 8 Reward: 949.8207999323495\n",
      "Episode: 9 Reward: 951.6973258918151\n"
     ]
    }
   ],
   "source": [
    "# load the environment\n",
    "env = suite.load(domain_name=\"walker\", task_name=\"walk\")\n",
    "# add wrappers onto the environment\n",
    "env = NormilizeActionSpecWrapper(env)\n",
    "env = MujocoActionNormalizer(environment=env, rescale='clip')\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    env_name = 'whatever'\n",
    "    policy = 'Gaussian'\n",
    "    eval = True\n",
    "    gamma = 0.99\n",
    "    tau = 0.005\n",
    "    lr = 0.0003\n",
    "    alpha = 0.2\n",
    "    automatic_entropy_tuning = True\n",
    "    seed = 42\n",
    "    batch_size = 512\n",
    "    num_steps = 1000000\n",
    "    hidden_size = 1024\n",
    "    updates_per_step = 1\n",
    "    start_steps = 10000\n",
    "    target_update_interval = 1\n",
    "    replay_size = 1000000\n",
    "    # use the cuda to speedup\n",
    "    # change back to True\n",
    "    cuda = False\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# get the dimensionality of the observation_spec after flattening\n",
    "flat_obs = tree.flatten(env.observation_spec())\n",
    "# combine all the shapes\n",
    "# obs_dim = sum([item.shape[0] for item in flat_obs])\n",
    "obs_dim = 0\n",
    "for i in flat_obs:\n",
    "    try:\n",
    "        obs_dim += i.shape[0]\n",
    "    except IndexError:\n",
    "        obs_dim += 1\n",
    "\n",
    "# setup agent, using Soft-Actor-Critic Model\n",
    "agent = SAC(obs_dim, env.action_spec(), args)\n",
    "\n",
    "# load checkpoint - UPLOAD YOUR FILE HERE!\n",
    "model_path = '../data/models/sac_checkpoint_walker_walk_batch512_hidden1024_1123_500'\n",
    "agent.load_checkpoint(model_path, evaluate=True)\n",
    "\n",
    "# pull out model\n",
    "model = agent.policy\n",
    "# setup hook dict\n",
    "hook_dict = init_hook_dict(model)\n",
    "# add hooks\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        print(name, module)\n",
    "        module.register_forward_hook(recordtodict_hook(name=name, hook_dict=hook_dict))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# run a few episodes just to collect activations\n",
    "num_episodes_to_run = 10\n",
    "\n",
    "for i in range(num_episodes_to_run):\n",
    "    time_step = env.reset()\n",
    "    episode_reward = 0\n",
    "    while not time_step.last():  # or env.get_termination()\n",
    "        # get the state\n",
    "        state = get_flat_obs(time_step)\n",
    "        # sample an action\n",
    "        action = agent.select_action(state)\n",
    "        time_step = env.step(action)\n",
    "\n",
    "        # record reward\n",
    "        episode_reward += time_step.reward\n",
    "    print('Episode: {} Reward: {}'.format(i, episode_reward))\n",
    "\n",
    "\n",
    "loaded_hook_dict = compile_hook_dict(hook_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear1': array([[-6.2950077 , -1.8400515 , -0.61484694, ..., -6.196599  ,\n",
       "         -4.020216  , -2.367227  ],\n",
       "        [-6.3720865 , -2.5290775 , -0.55355024, ..., -5.506919  ,\n",
       "         -3.7495894 , -1.7268317 ],\n",
       "        [-6.044528  , -2.2897046 , -1.4255805 , ..., -6.5680957 ,\n",
       "         -4.3975916 , -3.0558057 ],\n",
       "        ...,\n",
       "        [-7.6413584 , -0.2504742 ,  0.99779725, ..., -3.9121563 ,\n",
       "         -4.3732467 , -0.20234418],\n",
       "        [-7.681202  , -0.82079697,  1.5718853 , ..., -5.56293   ,\n",
       "         -3.7457004 , -1.5461395 ],\n",
       "        [-7.6753416 , -0.13907528,  0.13671899, ..., -5.111472  ,\n",
       "         -4.0749817 , -1.4309449 ]], dtype=float32),\n",
       " 'linear2': array([[-1.4720087 , -0.7104219 , -5.7952986 , ..., -1.180166  ,\n",
       "          0.44371885, -1.0896236 ],\n",
       "        [-1.6496288 , -0.6143804 , -5.4861    , ..., -1.1608174 ,\n",
       "          1.142792  , -1.2387209 ],\n",
       "        [-1.2699373 , -0.67645717, -4.7653494 , ..., -1.2263747 ,\n",
       "          0.5790795 , -0.97282654],\n",
       "        ...,\n",
       "        [-1.5489658 , -1.1593702 , -0.65446025, ..., -1.0146753 ,\n",
       "         -9.557172  , -0.8582494 ],\n",
       "        [-1.6531736 , -1.0374131 , -1.7110717 , ..., -1.1367586 ,\n",
       "         -8.596814  , -0.7753278 ],\n",
       "        [-1.3744001 , -0.8929526 , -2.2050261 , ..., -0.68902224,\n",
       "         -8.252254  , -0.5708902 ]], dtype=float32),\n",
       " 'mean_linear': array([[ 0.47874802, -0.33667922,  0.20797211, -0.5972399 , -0.32676524,\n",
       "          0.16908455],\n",
       "        [ 0.12121457, -0.320202  ,  0.09381254, -0.8443472 , -0.36035967,\n",
       "          0.18636283],\n",
       "        [ 0.7113609 , -0.31464833,  0.29398632, -0.35393614, -0.4803404 ,\n",
       "          0.15959035],\n",
       "        ...,\n",
       "        [-0.64069384,  0.19545904,  0.49027485, -0.3864278 ,  0.79146504,\n",
       "         -0.03469592],\n",
       "        [-0.8314685 ,  0.04952812,  0.46482256, -0.00366341,  0.47405174,\n",
       "         -0.16360357],\n",
       "        [-0.7126688 ,  0.7888616 ,  0.01081435,  0.04097991,  1.1313292 ,\n",
       "         -0.23462953]], dtype=float32),\n",
       " 'log_std_linear': array([[-0.31684044, -0.27128068, -0.15406375, -0.33387798, -0.3561815 ,\n",
       "         -0.20975652],\n",
       "        [-0.30472314, -0.23836106, -0.17740557, -0.406428  , -0.3488075 ,\n",
       "         -0.28604972],\n",
       "        [-0.33097583, -0.29412234, -0.13572036, -0.34201992, -0.393512  ,\n",
       "         -0.25766587],\n",
       "        ...,\n",
       "        [-0.47095034, -0.28126225, -0.24865603, -0.3132039 , -0.42429832,\n",
       "         -0.18717311],\n",
       "        [-0.47497743, -0.2250302 , -0.30017227, -0.1858749 , -0.38341084,\n",
       "         -0.15372646],\n",
       "        [-0.32877558, -0.3463034 , -0.2542297 , -0.18005928, -0.5204839 ,\n",
       "         -0.1526204 ]], dtype=float32)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_hook_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/matplotlib/axes/_base.py:3270: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  left, right = sorted([left, right], reverse=reverse)\n",
      "/opt/homebrew/lib/python3.10/site-packages/matplotlib/axes/_base.py:3652: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  bottom, top = sorted([bottom, top], reverse=reverse)\n",
      "/opt/homebrew/lib/python3.10/site-packages/matplotlib/axes/_base.py:3652: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  bottom, top = sorted([bottom, top], reverse=reverse)\n",
      "/opt/homebrew/lib/python3.10/site-packages/matplotlib/axes/_base.py:3270: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  left, right = sorted([left, right], reverse=reverse)\n",
      "/opt/homebrew/lib/python3.10/site-packages/matplotlib/axes/_base.py:3652: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  bottom, top = sorted([bottom, top], reverse=reverse)\n",
      "/opt/homebrew/lib/python3.10/site-packages/matplotlib/axes/_base.py:3270: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  left, right = sorted([left, right], reverse=reverse)\n",
      "/opt/homebrew/lib/python3.10/site-packages/matplotlib/axes/_base.py:3652: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  bottom, top = sorted([bottom, top], reverse=reverse)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x179ce2b30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cka_online = {'activation_1': [],\n",
    "                'activation_2': [],\n",
    "                'cka': []}\n",
    "\n",
    "# get combinations between activations\n",
    "for activation1 in loaded_hook_dict.keys():\n",
    "    for activation2 in loaded_hook_dict.keys():\n",
    "        cka_calc = cka(loaded_hook_dict[activation1], loaded_hook_dict[activation2])\n",
    "        # if activation1 == activation2:\n",
    "        #     cka_calc = 1\n",
    "        cka_online['cka'].append(cka_calc)\n",
    "        cka_online['activation_1'].append(activation1)\n",
    "        cka_online['activation_2'].append(activation2)\n",
    "\n",
    "df = pd.DataFrame(cka_online).pivot('activation_1', 'activation_2', 'cka')\n",
    "sns.heatmap(df, annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>activation_2</th>\n",
       "      <th>linear1</th>\n",
       "      <th>linear2</th>\n",
       "      <th>log_std_linear</th>\n",
       "      <th>mean_linear</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activation_1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linear1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.447772</td>\n",
       "      <td>0.343874</td>\n",
       "      <td>0.178376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear2</th>\n",
       "      <td>0.447772</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.131462</td>\n",
       "      <td>0.092108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_std_linear</th>\n",
       "      <td>0.343874</td>\n",
       "      <td>0.131462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.211454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_linear</th>\n",
       "      <td>0.178376</td>\n",
       "      <td>0.092108</td>\n",
       "      <td>0.211454</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "activation_2     linear1   linear2  log_std_linear  mean_linear\n",
       "activation_1                                                   \n",
       "linear1         1.000000  0.447772        0.343874     0.178376\n",
       "linear2         0.447772  1.000000        0.131462     0.092108\n",
       "log_std_linear  0.343874  0.131462        1.000000     0.211454\n",
       "mean_linear     0.178376  0.092108        0.211454     1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianPolicy(\n",
       "  (linear1): Linear(in_features=24, out_features=1024, bias=True)\n",
       "  (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (mean_linear): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  (log_std_linear): Linear(in_features=1024, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bc_net import BCNetworkContinuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BCNetworkContinuous:\n\tMissing key(s) in state_dict: \"fc3.weight\", \"fc3.bias\". \n\tUnexpected key(s) in state_dict: \"mean_linear.weight\", \"mean_linear.bias\", \"log_std_linear.weight\", \"log_std_linear.bias\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([256, 9]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb Cell 10\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X26sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m network \u001b[39m=\u001b[39m BCNetworkContinuous(obs_dim, env\u001b[39m.\u001b[39maction_spec()\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X26sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# load the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X26sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# network.load_state_dict(torch.load(args.model_path))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X26sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m network\u001b[39m.\u001b[39;49mload_state_dict(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X26sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m../data/bc_models/walker_1024_bc.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X26sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X26sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# set to eval mode\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X26sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m network\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BCNetworkContinuous:\n\tMissing key(s) in state_dict: \"fc3.weight\", \"fc3.bias\". \n\tUnexpected key(s) in state_dict: \"mean_linear.weight\", \"mean_linear.bias\", \"log_std_linear.weight\", \"log_std_linear.bias\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([1024, 24]) from checkpoint, the shape in current model is torch.Size([256, 9]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256])."
     ]
    }
   ],
   "source": [
    "from BCNetwork import BCNetwork\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# network = BCNetwork(24, 6, 1024).to(device)\n",
    "# network.load_state_dict(\n",
    "#     torch.load(\"../data/bc_models/walker_1024_bc.pt\", map_location=torch.device('cpu'))\n",
    "# )\n",
    "\n",
    "\n",
    "# # load the environment\n",
    "# if args.env_name == 'HalfCheetah-v4':\n",
    "#     env = suite.load(domain_name=\"cheetah\", task_name=\"run\")\n",
    "# else:\n",
    "#     raise NotImplementedError\n",
    "\n",
    "# add wrappers onto the environment\n",
    "env = NormilizeActionSpecWrapper(env)\n",
    "env = MujocoActionNormalizer(environment=env, rescale='clip')\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "\n",
    "# get the dimensionality of the observation_spec after flattening\n",
    "flat_obs = tree.flatten(env.observation_spec())\n",
    "# combine all the shapes\n",
    "\n",
    "# obs_dim = sum([item.shape[0] for item in flat_obs])\n",
    "\n",
    "for item in flat_obs:\n",
    "    temp = []\n",
    "    if len(item.shape) > 0:\n",
    "        temp.append(item.shape[0])\n",
    "obs_dim = sum(temp)\n",
    "\n",
    "# initialize the network\n",
    "network = BCNetworkContinuous(obs_dim, env.action_spec().shape[0])\n",
    "\n",
    "# load the model\n",
    "# network.load_state_dict(torch.load(args.model_path))\n",
    "network.load_state_dict(\n",
    "    torch.load(\"../data/bc_models/walker_1024_bc.pt\", map_location=torch.device('cpu'))\n",
    ")\n",
    "\n",
    "# set to eval mode\n",
    "network.eval()\n",
    "\n",
    "# setup hook dict\n",
    "hook_dict = init_hook_dict(network)\n",
    "# add hooks\n",
    "for name, module in network.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        module.register_forward_hook(recordtodict_hook(name=name, hook_dict=hook_dict))\n",
    "\n",
    "\n",
    "\n",
    "# run a few episodes just to collect activations\n",
    "num_episodes_to_run = args.num_episodes\n",
    "\n",
    "for i in range(num_episodes_to_run):\n",
    "    time_step = env.reset()\n",
    "    episode_reward = 0\n",
    "    while not time_step.last():  # or env.get_termination()\n",
    "        # get the state\n",
    "        state = get_flat_obs(time_step)\n",
    "        # add batch dimension\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        # sample an action\n",
    "        tensor_state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = network(tensor_state).detach().numpy()\n",
    "        time_step = env.step(action)\n",
    "    print('Episode: {} Reward: {}'.format(i, episode_reward))\n",
    "\n",
    "### optional: save + load the hook_dict\n",
    "\n",
    "# make folder from args.save_path\n",
    "os.makedirs(args.save_path, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(args.save_path, 'hook_dict.npy')\n",
    "save_hook_dict(hook_dict, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_hook_dict() missing 1 required positional argument: 'load_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb Cell 8\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m activations\u001b[39m.\u001b[39;49mload_hook_dict()\n",
      "\u001b[0;31mTypeError\u001b[0m: load_hook_dict() missing 1 required positional argument: 'load_path'"
     ]
    }
   ],
   "source": [
    "bc_nonoise_model_activations_path = \"../data/activations/cheetah_123456_10000_nonoise_bcmodel\"\n",
    "bc_nonoise_hooks_path = bc_nonoise_model_activations_path + \"/hook_dict.npy\"\n",
    "\n",
    "\n",
    "activations.load_hook_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_network(network, num_episodes=100, deterministic=True):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action = network(torch.tensor(obs, dtype=torch.float32)).argmax().item()\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step([action])\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb Cell 10\u001b[0m in \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         module\u001b[39m.\u001b[39mregister_forward_hook(recordtodict_hook(name\u001b[39m=\u001b[39mname, hook_dict\u001b[39m=\u001b[39mhook_dict_bc))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# # run a few episodes to collect activations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# num_episodes_to_run = 10\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m#     return mean_episode_reward\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m evaluate_network(network)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m loaded_hook_dict_bc \u001b[39m=\u001b[39m compile_hook_dict(hook_dict_bc)\n",
      "\u001b[1;32m/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# _states are only useful when using LSTM policies\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     action \u001b[39m=\u001b[39m network(torch\u001b[39m.\u001b[39;49mtensor(obs, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32))\u001b[39m.\u001b[39margmax()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# here, action, rewards and dones are arrays\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# because we are using vectorized env\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielson/DSC180B-A08/visualizations/plot_cka.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep([action])\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "from BCNetwork import BCNetwork\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "network = BCNetwork(24, 6, 1024).to(device)\n",
    "network.load_state_dict(\n",
    "    torch.load(\"../data/bc_models/walker_1024_bc.pt\", map_location=torch.device('cpu'))\n",
    ")\n",
    "\n",
    "# setup hook dict\n",
    "hook_dict_bc = init_hook_dict(network)\n",
    "# add hooks\n",
    "for name, module in network.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        module.register_forward_hook(recordtodict_hook(name=name, hook_dict=hook_dict_bc))\n",
    "\n",
    "\n",
    "# # run a few episodes to collect activations\n",
    "# num_episodes_to_run = 10\n",
    "\n",
    "# for i in range(num_episodes_to_run):\n",
    "#     time_step = env.reset()\n",
    "#     episode_reward = 0\n",
    "#     while not time_step.last():\n",
    "#         # get the state\n",
    "#         state = get_flat_obs(time_step)\n",
    "\n",
    "\n",
    "#         # sample an action\n",
    "        \n",
    "#         def select_action(self, state, evaluate=False):\n",
    "#             state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "#             if evaluate is False:\n",
    "#                 action, _, _ = self.policy.sample(state)\n",
    "#             else:\n",
    "#                 _, _, action = self.policy.sample(state)\n",
    "#             return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "#         action = network.select_action(state)\n",
    "\n",
    "\n",
    "#         time_step = env.step(action)\n",
    "\n",
    "#         # record reward\n",
    "#         episode_reward += time_step.reward\n",
    "        \n",
    "#     print('Episode: {} Reward: {}'.format(i, episode_reward))\n",
    "\n",
    "\n",
    "# def evaluate_network(network, num_episodes=10):\n",
    "#     \"\"\"\n",
    "#     Evaluate a RL agent\n",
    "#     :param model: (BaseRLModel object) the RL Agent\n",
    "#     :param num_episodes: (int) number of episodes to evaluate it\n",
    "#     :return: (float) Mean reward for the last num_episodes\n",
    "#     \"\"\"\n",
    "#     all_episode_rewards = []\n",
    "#     for i in range(num_episodes):\n",
    "#         episode_rewards = []\n",
    "#         done = False\n",
    "\n",
    "\n",
    "#         # flat_obs = tree.flatten(env.observation_spec())\n",
    "#         # # combine all the shapes\n",
    "#         # # obs_dim = sum([item.shape[0] for item in flat_obs])\n",
    "#         # obs_dim = 0\n",
    "#         # for i in flat_obs:\n",
    "#         #     try:\n",
    "#         #         obs_dim += i.shape[0]\n",
    "#         #     except IndexError:\n",
    "#         #         obs_dim += 1\n",
    "\n",
    "\n",
    "#         obs = env.reset()\n",
    "        \n",
    "#         while not done:\n",
    "       \n",
    "\n",
    "#             action = network(torch.tensor(list(obs[3].values()), dtype=torch.float32))\n",
    "#             obs, reward, done, info = env.step([action])\n",
    "#             episode_rewards.append(reward)\n",
    "\n",
    "\n",
    "\n",
    "#         all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "#     mean_episode_reward = np.mean(all_episode_rewards)\n",
    "#     print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
    "\n",
    "#     return mean_episode_reward\n",
    "\n",
    "evaluate_network(network)\n",
    "\n",
    "loaded_hook_dict_bc = compile_hook_dict(hook_dict_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (3374787892.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [16], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    torch.tensor(list(obs[3].values()), dtype=torch.float32))\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "torch.tensor(list(obs[3].values()), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<StepType.FIRST: 0>, reward=None, discount=None, observation=OrderedDict([('orientations', array([ 0.288338  , -0.9575287 , -0.719978  , -0.6939969 ,  0.8276786 ,\n",
       "       -0.5612024 ,  0.9359683 , -0.35208428,  0.27002808, -0.9628525 ,\n",
       "        0.983658  , -0.18004684,  0.9929495 ,  0.1185384 ], dtype=float32)), ('height', array(1.3, dtype=float32)), ('velocity', array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('orientations',\n",
       "              array([ 0.288338  , -0.9575287 , -0.719978  , -0.6939969 ,  0.8276786 ,\n",
       "                     -0.5612024 ,  0.9359683 , -0.35208428,  0.27002808, -0.9628525 ,\n",
       "                      0.983658  , -0.18004684,  0.9929495 ,  0.1185384 ], dtype=float32)),\n",
       "             ('height', array(1.3, dtype=float32)),\n",
       "             ('velocity',\n",
       "              array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc1': [], 'fc2': [], 'mean_linear': [], 'log_std_linear': []}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_hook_dict_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCNetwork(\n",
       "  (fc1): Linear(in_features=24, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (mean_linear): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  (log_std_linear): Linear(in_features=1024, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
