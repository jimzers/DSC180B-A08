{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a074ac1-6e6f-40d9-8631-6dcb92d2f2aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 12 11:11:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    42W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Installing dm_control...\n",
      "env: MUJOCO_GL=egl\n",
      "Checking that the dm_control installation succeeded...\n",
      "Installed dm_control 1.0.8\n",
      "rm: cannot remove '=1.0.8': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import distutils.util\n",
    "import subprocess\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "    raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "print('Installing dm_control...')\n",
    "#!pip install -q dm_control>=1.0.8\n",
    "\n",
    "# Configure dm_control to use the EGL rendering backend (requires GPU)\n",
    "%env MUJOCO_GL=egl\n",
    "\n",
    "print('Checking that the dm_control installation succeeded...')\n",
    "try:\n",
    "    from dm_control import suite\n",
    "    env = suite.load('cartpole', 'swingup')\n",
    "    pixels = env.physics.render()\n",
    "except Exception as e:\n",
    "    raise e from RuntimeError(\n",
    "      'Something went wrong during installation. Check the shell output above '\n",
    "      'for more information.\\n'\n",
    "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "else:\n",
    "    del pixels, suite\n",
    "\n",
    "!echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")\n",
    "\n",
    "# %pip -q install git+https://github.com/deepmind/acme.git#egg=dm-acme[jax,tf,envs]\n",
    "# %pip -q install imageio-ffmpeg\n",
    "# %pip -q install gdown\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "# Removed unnecessary generated file\n",
    "! rm -r \"=1.0.8\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tree\n",
    "# plot the activations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.optim import Adam\n",
    "\n",
    "#Run to install MuJoCo and `dm_control`\n",
    "# import this first to resolve the issue.\n",
    "import sys\n",
    "sys.path.insert(1, '../source/')\n",
    "from acme import wrappers\n",
    "from model import *\n",
    "from utils import *\n",
    "# Soft-Actor-Critic Model\n",
    "from sac import *\n",
    "from replay_memory import *\n",
    "\n",
    "# try out the wrappers\n",
    "from acme import wrappers\n",
    "from dm_control import suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f2f851-ad95-423a-a01d-69102ecf48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Environment wrappers\n",
    "from dm_env import specs\n",
    "\n",
    "\n",
    "# environment wrappers\n",
    "class NormilizeActionSpecWrapper(wrappers.EnvironmentWrapper):\n",
    "    \"\"\"Turn each dimension of the actions into the range of [-1, 1].\"\"\"\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "\n",
    "        action_spec = environment.action_spec()\n",
    "        self._scale = action_spec.maximum - action_spec.minimum\n",
    "        self._offset = action_spec.minimum\n",
    "\n",
    "        minimum = action_spec.minimum * 0 - 1.\n",
    "        maximum = action_spec.minimum * 0 + 1.\n",
    "        self._action_spec = specs.BoundedArray(\n",
    "            action_spec.shape,\n",
    "            action_spec.dtype,\n",
    "            minimum,\n",
    "            maximum,\n",
    "            name=action_spec.name)\n",
    "\n",
    "    def _from_normal_actions(self, actions):\n",
    "        actions = 0.5 * (actions + 1.0)  # a_t is now in the range [0, 1]\n",
    "        # scale range to [minimum, maximum]\n",
    "        return actions * self._scale + self._offset\n",
    "\n",
    "    def step(self, action):\n",
    "        action = self._from_normal_actions(action)\n",
    "        return self._environment.step(action)\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "\n",
    "class MujocoActionNormalizer(wrappers.EnvironmentWrapper):\n",
    "    \"\"\"Rescale actions to [-1, 1] range for mujoco physics engine.\n",
    "\n",
    "    For control environments whose actions have bounded range in [-1, 1], this\n",
    "      adaptor rescale actions to the desired range. This allows actor network to\n",
    "      output unscaled actions for better gradient dynamics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, environment, rescale='clip'):\n",
    "        super().__init__(environment)\n",
    "        self._rescale = rescale\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Rescale actions to [-1, 1] range before stepping wrapped environment.\"\"\"\n",
    "        if self._rescale == 'tanh':\n",
    "            scaled_actions = tree.map_structure(np.tanh, action)\n",
    "        elif self._rescale == 'clip':\n",
    "            scaled_actions = tree.map_structure(lambda a: np.clip(a, -1., 1.), action)\n",
    "        else:\n",
    "            raise ValueError('Unrecognized scaling option: %s' % self._rescale)\n",
    "        return self._environment.step(scaled_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656d547c-0951-4486-87a8-d52d7903b9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('orientations', Array(shape=(14,), dtype=dtype('float32'), name='orientations')), ('height', Array(shape=(), dtype=dtype('float32'), name='height')), ('velocity', Array(shape=(9,), dtype=dtype('float32'), name='velocity'))])\n",
      "[Array(shape=(14,), dtype=dtype('float32'), name='orientations'), Array(shape=(9,), dtype=dtype('float32'), name='velocity')]\n",
      "Loading models from sac_checkpoint_walker_walk_batch512_hidden1024_1123_500\n"
     ]
    }
   ],
   "source": [
    "# load the environment\n",
    "env = suite.load(domain_name=\"walker\", task_name=\"walk\")\n",
    "# add wrappers onto the environment\n",
    "env = NormilizeActionSpecWrapper(env)\n",
    "env = MujocoActionNormalizer(environment=env, rescale='clip')\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "\n",
    "\n",
    "class Args:\n",
    "    env_name = 'whatever'\n",
    "    policy = 'Gaussian'\n",
    "    eval = True\n",
    "    gamma = 0.99\n",
    "    tau = 0.005\n",
    "    lr = 0.0003\n",
    "    alpha = 0.2\n",
    "    automatic_entropy_tuning = True\n",
    "    seed = 42\n",
    "    batch_size = 512\n",
    "    num_steps = 1000000\n",
    "    hidden_size = 1024\n",
    "    updates_per_step = 1\n",
    "    start_steps = 10000\n",
    "    target_update_interval = 1\n",
    "    replay_size = 1000000\n",
    "    cuda = True\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# get the dimensionality of the observation_spec after flattening\n",
    "print(env.observation_spec())\n",
    "flat_obs = tree.flatten(env.observation_spec())\n",
    "#flat_obs = flat_obs[1:]\n",
    "print(flat_obs[1:])\n",
    "# combine all the shapes\n",
    "#obs_dim = sum([item.shape[0] for item in flat_obs])\n",
    "obs_dim = 0\n",
    "for i in flat_obs:\n",
    "    try:\n",
    "        obs_dim += i.shape[0]\n",
    "    except IndexError:\n",
    "        obs_dim += 1\n",
    "# setup agent, using Soft-Actor-Critic Model\n",
    "agent = SAC(obs_dim, env.action_spec(), args)\n",
    "# load checkpoint - UPLOAD YOUR FILE HERE!\n",
    "model_path = 'sac_checkpoint_walker_walk_batch512_hidden1024_1123_500'\n",
    "agent.load_checkpoint(model_path, evaluate=True)\n",
    "\n",
    "# pull out model\n",
    "model = agent.policy\n",
    "# setup hook dict\n",
    "hook_dict = init_hook_dict(model)\n",
    "# add hooks\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        module.register_forward_hook(recordtodict_hook(name=name, hook_dict=hook_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbce47db-f3a0-4855-9f88-037a619ab639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array(shape=(), dtype=dtype('float32'), name='height'),\n",
       " Array(shape=(14,), dtype=dtype('float32'), name='orientations'),\n",
       " Array(shape=(9,), dtype=dtype('float32'), name='velocity')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "219f0bb3-2d30-4cce-ae47-1fbc03599cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Reward: 956.8336200937629\n",
      "Episode: 11 Reward: 932.5251256353222\n",
      "Episode: 21 Reward: 953.6426235754043\n",
      "Episode: 31 Reward: 982.009455576539\n",
      "Episode: 41 Reward: 939.4821482943371\n"
     ]
    }
   ],
   "source": [
    "# run a few episodes just to collect activations\n",
    "num_episodes_to_run = 42\n",
    "\n",
    "rewards = []\n",
    "states = []\n",
    "actions = []\n",
    "for i in range(num_episodes_to_run):\n",
    "    time_step = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not time_step.last():  # or env.get_termination()\n",
    "        # get the state\n",
    "        #state = get_flat_obs(time_step)\n",
    "        flat_obs = tree.flatten(time_step.observation)\n",
    "        flat_obs[0] = flat_obs[0].reshape(-1,1)[0]\n",
    "        state = np.concatenate(flat_obs)\n",
    "        # sample an action\n",
    "        action = agent.select_action(state)\n",
    "        time_step = env.step(action)\n",
    "\n",
    "        # record reward\n",
    "        time_step_reward = time_step.reward\n",
    "        rewards.append(time_step_reward)\n",
    "        episode_reward += time_step_reward\n",
    "        \n",
    "        #record states and actions\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "    if i % 10 == 0:\n",
    "        print('Episode: {} Reward: {}'.format(i+1, episode_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
