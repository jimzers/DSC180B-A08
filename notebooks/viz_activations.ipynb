{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9V0asdX-04f",
    "outputId": "38ee62f2-bf6d-4c00-a6b6-eb729c7609a1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make our custome src accessible\n",
    "import sys\n",
    "sys.path.insert(0,'../src')\n",
    "\n",
    "#Run to install MuJoCo and `dm_control`\n",
    "import distutils.util\n",
    "import subprocess\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "    raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "print('Installing dm_control...')\n",
    "!pip install -q dm_control>=1.0.8\n",
    "\n",
    "# Configure dm_control to use the EGL rendering backend (requires GPU)\n",
    "%env MUJOCO_GL=egl\n",
    "\n",
    "print('Checking that the dm_control installation succeeded...')\n",
    "try:\n",
    "    from dm_control import suite\n",
    "    env = suite.load('cartpole', 'swingup')\n",
    "    pixels = env.physics.render()\n",
    "except Exception as e:\n",
    "    raise e from RuntimeError(\n",
    "      'Something went wrong during installation. Check the shell output above '\n",
    "      'for more information.\\n'\n",
    "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "else:\n",
    "    del pixels, suite\n",
    "\n",
    "!echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")\n",
    "\n",
    "# %pip -q install git+https://github.com/deepmind/acme.git#egg=dm-acme[jax,tf,envs]\n",
    "# %pip -q install imageio-ffmpeg\n",
    "# %pip -q install gdown\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "# Removed unnecessary generated file\n",
    "! rm -r \"=1.0.8\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tree\n",
    "# plot the activations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.optim import Adam\n",
    "\n",
    "# import this first to resolve the issue.\n",
    "from acme import wrappers\n",
    "from model import *\n",
    "from utils import *\n",
    "# Soft-Actor-Critic Model\n",
    "from sac import *\n",
    "from replay_memory import *\n",
    "\n",
    "# try out the wrappers\n",
    "from acme import wrappers\n",
    "from dm_control import suite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "HbxFI8NI-srX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Environment wrappers\n",
    "from dm_env import specs\n",
    "\n",
    "\n",
    "# environment wrappers\n",
    "class NormilizeActionSpecWrapper(wrappers.EnvironmentWrapper):\n",
    "    \"\"\"Turn each dimension of the actions into the range of [-1, 1].\"\"\"\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "\n",
    "        action_spec = environment.action_spec()\n",
    "        self._scale = action_spec.maximum - action_spec.minimum\n",
    "        self._offset = action_spec.minimum\n",
    "\n",
    "        minimum = action_spec.minimum * 0 - 1.\n",
    "        maximum = action_spec.minimum * 0 + 1.\n",
    "        self._action_spec = specs.BoundedArray(\n",
    "            action_spec.shape,\n",
    "            action_spec.dtype,\n",
    "            minimum,\n",
    "            maximum,\n",
    "            name=action_spec.name)\n",
    "\n",
    "    def _from_normal_actions(self, actions):\n",
    "        actions = 0.5 * (actions + 1.0)  # a_t is now in the range [0, 1]\n",
    "        # scale range to [minimum, maximum]\n",
    "        return actions * self._scale + self._offset\n",
    "\n",
    "    def step(self, action):\n",
    "        action = self._from_normal_actions(action)\n",
    "        return self._environment.step(action)\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "\n",
    "class MujocoActionNormalizer(wrappers.EnvironmentWrapper):\n",
    "    \"\"\"Rescale actions to [-1, 1] range for mujoco physics engine.\n",
    "\n",
    "    For control environments whose actions have bounded range in [-1, 1], this\n",
    "      adaptor rescale actions to the desired range. This allows actor network to\n",
    "      output unscaled actions for better gradient dynamics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, environment, rescale='clip'):\n",
    "        super().__init__(environment)\n",
    "        self._rescale = rescale\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Rescale actions to [-1, 1] range before stepping wrapped environment.\"\"\"\n",
    "        if self._rescale == 'tanh':\n",
    "            scaled_actions = tree.map_structure(np.tanh, action)\n",
    "        elif self._rescale == 'clip':\n",
    "            scaled_actions = tree.map_structure(lambda a: np.clip(a, -1., 1.), action)\n",
    "        else:\n",
    "            raise ValueError('Unrecognized scaling option: %s' % self._rescale)\n",
    "        return self._environment.step(scaled_actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions Moved to `utils.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwPJvZhfIRGO"
   },
   "source": [
    "## Environment and agent setup\n",
    "\n",
    "**NOTE: Make sure you download the pretrained weights or upload your own weights before running this cell!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTgqlqlT-srY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the environment\n",
    "env = suite.load(domain_name=\"walker\", task_name=\"walk\")\n",
    "# add wrappers onto the environment\n",
    "env = NormilizeActionSpecWrapper(env)\n",
    "env = MujocoActionNormalizer(environment=env, rescale='clip')\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "\n",
    "\n",
    "class Args:\n",
    "    env_name = 'whatever'\n",
    "    policy = 'Gaussian'\n",
    "    eval = True\n",
    "    gamma = 0.99\n",
    "    tau = 0.005\n",
    "    lr = 0.0003\n",
    "    alpha = 0.2\n",
    "    automatic_entropy_tuning = True\n",
    "    seed = 42\n",
    "    batch_size = 512\n",
    "    num_steps = 1000000\n",
    "    hidden_size = 1024\n",
    "    updates_per_step = 1\n",
    "    start_steps = 10000\n",
    "    target_update_interval = 1\n",
    "    replay_size = 1000000\n",
    "    # use the cuda to speedup\n",
    "    cuda = True\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# get the dimensionality of the observation_spec after flattening\n",
    "flat_obs = tree.flatten(env.observation_spec())\n",
    "# combine all the shapes\n",
    "# obs_dim = sum([item.shape[0] for item in flat_obs])\n",
    "obs_dim = 0\n",
    "for i in flat_obs:\n",
    "    try:\n",
    "        obs_dim += i.shape[0]\n",
    "    except IndexError:\n",
    "        obs_dim += 1\n",
    "\n",
    "# setup agent, using Soft-Actor-Critic Model\n",
    "agent = SAC(obs_dim, env.action_spec(), args)\n",
    "# load checkpoint - UPLOAD YOUR FILE HERE!\n",
    "model_path = '../data/models/sac_checkpoint_walker_walk_batch512_hidden1024_1123_500'\n",
    "agent.load_checkpoint(model_path, evaluate=True)\n",
    "\n",
    "# pull out model\n",
    "model = agent.policy\n",
    "# setup hook dict\n",
    "hook_dict = init_hook_dict(model)\n",
    "# add hooks\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        module.register_forward_hook(recordtodict_hook(name=name, hook_dict=hook_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbG5D_afIX-q"
   },
   "source": [
    "## Collecting activations and kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAH21fM0-srZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "WALKER_GEOM_NAMES = ['floor', 'torso', 'right_thigh', 'right_leg',\n",
    "                      'right_foot', 'left_thigh', 'left_leg', 'left_foot']\n",
    "WALKER_JOINT_NAMES = ['rootz', 'rootx', 'rooty', 'right_hip', 'right_knee', 'right_ankle',\n",
    "                      'left_hip', 'left_knee', 'left_ankle']\n",
    "WALKER_ACTUATOR_NAMES = ['right_hip', 'right_knee', 'right_ankle',\n",
    "                         'left_hip', 'left_knee', 'left_ankle']\n",
    "\n",
    "# get the mapping of the geom names\n",
    "geom_names_to_idx = {geom_name: idx for idx, geom_name in enumerate(WALKER_GEOM_NAMES)}\n",
    "# get the mapping of the joint names\n",
    "joint_names_to_idx = {joint_name: idx for idx, joint_name in enumerate(WALKER_JOINT_NAMES)}\n",
    "# get the mapping of the actuator names\n",
    "actuator_names_to_idx = {actuator_name: idx for idx, actuator_name in enumerate(WALKER_ACTUATOR_NAMES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HdaU6d3-srZ",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run a few episodes just to collect activations\n",
    "num_episodes_to_run = 42\n",
    "\n",
    "# for recording kinematics\n",
    "total_kinematic_dict = {\n",
    "    'geom_positions': [],\n",
    "    'joint_angles': [],\n",
    "    'joint_velocities': [],\n",
    "    'actuator_forces': []\n",
    "}\n",
    "\n",
    "for i in range(num_episodes_to_run):\n",
    "    time_step = env.reset()\n",
    "    episode_reward = 0\n",
    "    while not time_step.last():  # or env.get_termination()\n",
    "        # get the state\n",
    "        state = get_flat_obs(time_step)\n",
    "        # sample an action\n",
    "        action = agent.select_action(state)\n",
    "        time_step = env.step(action)\n",
    "\n",
    "        # record kinematics\n",
    "        kinematic_dict = get_kinematics(env.physics, WALKER_GEOM_NAMES, WALKER_JOINT_NAMES, WALKER_ACTUATOR_NAMES)\n",
    "        total_kinematic_dict['geom_positions'].append(kinematic_dict['geom_positions'])\n",
    "        total_kinematic_dict['joint_angles'].append(kinematic_dict['joint_angles'])\n",
    "        total_kinematic_dict['joint_velocities'].append(kinematic_dict['joint_velocities'])\n",
    "        total_kinematic_dict['actuator_forces'].append(kinematic_dict['actuator_forces'])\n",
    "        # record reward\n",
    "        episode_reward += time_step.reward\n",
    "    if i % 10 == 0:\n",
    "        print('Episode: {} Reward: {}'.format(i+1, episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ho8eGUoYl4WT"
   },
   "outputs": [],
   "source": [
    "#### optional: save + load the hook_dict\n",
    "# save_path = 'hook_dict.npy'\n",
    "# save_hook_dict(hook_dict, save_path)\n",
    "#\n",
    "# load_path = 'hook_dict.npy'\n",
    "# loaded_hook_dict = load_hook_dict(load_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BIAKnOuIdUj"
   },
   "outputs": [],
   "source": [
    "# otherwise, just compile the hook_dict\n",
    "loaded_hook_dict = compile_hook_dict(hook_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N52-j9i1l7MW"
   },
   "outputs": [],
   "source": [
    "# process the kinematics - convert the kinematics to numpy arrays\n",
    "total_kinematic_dict['geom_positions'] = np.stack(total_kinematic_dict['geom_positions'],\n",
    "                                                  axis=0)  # combine the geom_positions_arr into (t, n, 3)\n",
    "total_kinematic_dict['joint_angles'] = np.array(total_kinematic_dict['joint_angles'])\n",
    "total_kinematic_dict['joint_velocities'] = np.array(total_kinematic_dict['joint_velocities'])\n",
    "total_kinematic_dict['actuator_forces'] = np.array(total_kinematic_dict['actuator_forces'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F4fZegfIi_s"
   },
   "source": [
    "## Example: Using PCA to visualize activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_hook_dict[\"linear1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pojGg6V-sra",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# do pca on the activations\n",
    "pca_dict = {}\n",
    "for name, arr in loaded_hook_dict.items():\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(arr)\n",
    "    pca_dict[name] = pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygH8XD15-sra",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### optional: save + load the pca_dict\n",
    "# save_path = 'pca_dict.npy'\n",
    "# save_pca_dict(pca_dict, save_path)\n",
    "# loaded_pca_dict = load_pca_dict(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qO7HAp0-srb",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# plot the activations, and save the activations to a dictionary\n",
    "os.makedirs('activations', exist_ok=True)\n",
    "\n",
    "activations_dict = {}\n",
    "for name, pca in pca_dict.items():\n",
    "    # get activations\n",
    "    activations = get_activations(pca_dict=pca_dict, compiled_hook_dict=loaded_hook_dict, layer_name=name)\n",
    "\n",
    "    # save activations\n",
    "    activations_dict[name] = activations\n",
    "\n",
    "    # plot activations\n",
    "    save_path = 'activations/{}.png'.format(name)\n",
    "    fig_im = plot_activations(activations, layer_name=name, save_path=save_path, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q4pAgVgmC48"
   },
   "source": [
    "## Example: Using Power Spectral Density to visualize activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaxCHrzymCqv"
   },
   "outputs": [],
   "source": [
    "# filter the activations_dict to only include the layers we want\n",
    "layers_to_include = ['linear1', 'linear2']\n",
    "\n",
    "# get the activations for the layers we want\n",
    "filtered_activations = {k: v for k, v in loaded_hook_dict.items() if k in layers_to_include}\n",
    "\n",
    "# split the activations by the number of episodes collected (assumes all episodes have the same number of steps)\n",
    "# get the number of steps per episode\n",
    "num_steps_per_episode = activations_dict['linear1'].shape[0] // num_episodes_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEy4Kw_NmFsf"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "# get the power spectral density of the pca of the activations\n",
    "os.makedirs('psd', exist_ok=True)\n",
    "\n",
    "k = 10\n",
    "psd_trajectory_dict = {}\n",
    "for name, activations in filtered_activations.items():\n",
    "    # get the pca of the activations to k dimensions\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(activations)\n",
    "    activations_pca = pca.transform(activations)  # (num_episodes_to_run * num_steps_per_episode, k)\n",
    "\n",
    "    activations_reshaped = activations_pca.reshape(num_episodes_to_run, num_steps_per_episode,\n",
    "                                                   -1)  # (episodes, steps, k)\n",
    "    f, psd = scipy.signal.welch(activations_reshaped, fs=50, nperseg=500, noverlap=250, axis=-2)\n",
    "\n",
    "    psd_pca = np.average(psd, axis=-1, weights=pca.explained_variance_ratio_)  # (episodes, steps)\n",
    "    psd_trajectory = np.mean(psd_pca, axis=0)  # (steps,)\n",
    "\n",
    "    psd_trajectory_dict[name] = psd_trajectory\n",
    "\n",
    "    # make a plot of the power spectral density\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogy(f, psd_trajectory)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('frequency [Hz]')\n",
    "    ax.set_ylabel('PSD [V**2/Hz]')\n",
    "    # set x scale to log\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "    save_path = 'psd/{}.png'.format(name)\n",
    "    # save_path = None\n",
    "    fig_im = fig2img(fig)\n",
    "    if save_path is not None:\n",
    "        fig_im.save(save_path)\n",
    "\n",
    "# make a plot of the power spectral density of all the layers from psd_trajectory_dict\n",
    "fig, ax = plt.subplots()\n",
    "for name, psd_trajectory in psd_trajectory_dict.items():\n",
    "    ax.semilogy(f, psd_trajectory, label=name)\n",
    "ax.set_title('All layers')\n",
    "ax.set_xlabel('frequency [Hz]')\n",
    "ax.set_ylabel('PSD [V**2/Hz]')\n",
    "# set x scale to log\n",
    "ax.set_xscale('log')\n",
    "ax.legend()\n",
    "\n",
    "save_path = 'psd/all_layers.png'\n",
    "# save_path = None\n",
    "fig_im = fig2img(fig)\n",
    "if save_path is not None:\n",
    "    fig_im.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vDOxoO-mJML"
   },
   "source": [
    "### Task: Implement PCA / PSD over kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(X, name):\n",
    "    \"\"\"\n",
    "    Helper function that plot the data using dimensionality reduction method\n",
    "    \"\"\"\n",
    "    os.makedirs(\"pca/\", exist_ok=True)\n",
    "    pca = PCA(n_components=2)\n",
    "    trans = pca.fit_transform(X)\n",
    "    plt.scatter(trans[:, 0], trans[:, 1], s=1)\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"First PC\")\n",
    "    plt.ylabel(\"Second PC\")\n",
    "    plt.savefig(f\"pca/{name}.png\", dpi=200)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are periodic patterns in the PCA image of the X axis, which indicates that it is coordinating sequential periodic actions sequence between each geometric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xt3g7ubAmJmo"
   },
   "outputs": [],
   "source": [
    "X = total_kinematic_dict[\"geom_positions\"][:,:,0]\n",
    "plot_pca(X, \"X_axis_of_geom_position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total_kinematic_dict[\"geom_positions\"][:,:,2]\n",
    "plot_pca(X, \"Z_axis_of_geom_position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total_kinematic_dict[\"joint_angles\"]\n",
    "plot_pca(X, \"joint_angels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total_kinematic_dict[\"joint_velocities\"]\n",
    "plot_pca(X, \"joint_velocities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = total_kinematic_dict[\"actuator_forces\"]\n",
    "plot_pca(X, \"actuator_forces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use tSNE to Cluster and visualize data \n",
    "`sklearn` provides implementation to tSNE in manifold package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(trans, name):\n",
    "    os.makedirs(\"tsne/\", exist_ok=True)\n",
    "    plt.scatter(trans[:, 0], trans[:, 1], s=1)\n",
    "    plt.title(name)\n",
    "    plt.savefig(f\"tsne/{name}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = total_kinematic_dict[\"geom_positions\"][:,:,2]\n",
    "tsne = TSNE(init=\"pca\",learning_rate=\"auto\", n_jobs=-1)\n",
    "trans = tsne.fit_transform(X)\n",
    "plot_tsne(trans, \"Perplexity=30 Z geom_positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity maybe too low? to show the inner cluster that is maybe meaningless? try perplexity 50 in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total_kinematic_dict[\"geom_positions\"][:,:,2]\n",
    "tsne = TSNE(init=\"pca\",learning_rate=\"auto\", perplexity=50, n_jobs=-1)\n",
    "trans = tsne.fit_transform(X)\n",
    "plot_tsne(trans, \"Perplexity=50 Z geom_positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = total_kinematic_dict[\"geom_positions\"][:,:,0]\n",
    "tsne = TSNE(init=\"pca\",learning_rate=\"auto\", n_jobs=-1)\n",
    "trans = tsne.fit_transform(X)\n",
    "plot_tsne(trans, \"Perplexity=30 X geom_positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total_kinematic_dict[\"geom_positions\"][:,:,0]\n",
    "tsne = TSNE(init=\"pca\",learning_rate=\"auto\", perplexity=lexity=50, n_jobs=-1)\n",
    "trans = tsne.fit_transform(X)\n",
    "plot_tsne(trans, \"Perplexity=50 X geom_positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9161SUb-srb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plot in a live loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZxJjdlU-src",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot a single point\n",
    "\n",
    "time_step = env.reset()\n",
    "episode_len = 0\n",
    "episode_reward = 0\n",
    "\n",
    "# create an imageio writer to a mp4\n",
    "writer = imageio.get_writer('test_combined_walker.mp4', fps=30)\n",
    "\n",
    "# first, clear the hooks\n",
    "clear_hook_dict(hook_dict)\n",
    "\n",
    "while not time_step.last() and episode_len < 1000:\n",
    "    # get the state\n",
    "    state = get_flat_obs(time_step)\n",
    "    # sample an action\n",
    "    action = agent.select_action(state)\n",
    "    # step the environment\n",
    "    time_step = env.step(action)\n",
    "\n",
    "    # record from a camera\n",
    "    stitched_img = create_stitched_img(env)\n",
    "\n",
    "    # get the latest dimension and do pca\n",
    "    for name, hook_list in hook_dict.items():\n",
    "        latest_activation = hook_list[-1]\n",
    "        point = latest_activation.detach().cpu().numpy().reshape(1, -1)\n",
    "        pca = pca_dict[name]\n",
    "        # grab corresponding activations\n",
    "        activations = activations_dict[name]\n",
    "\n",
    "        # plot single point\n",
    "        img = plot_single_point(point=point, activations=activations, pca=pca, layer_name=name)\n",
    "\n",
    "        # combine the two images\n",
    "        stitched_img = np.concatenate((img, stitched_img), axis=1)\n",
    "\n",
    "        # img_arr.append(combined_img)\n",
    "\n",
    "    # write to the writer\n",
    "    writer.append_data(stitched_img)\n",
    "\n",
    "    # logging stuff\n",
    "    episode_reward += time_step.reward\n",
    "    episode_len += 1\n",
    "\n",
    "    # # clear the hooks for memory purposes?\n",
    "    # clear_hook_dict(hook_dict)\n",
    "\n",
    "# close the writer\n",
    "writer.close()\n",
    "\n",
    "print('episode stats:', 'episode_len:', episode_len, 'episode_reward:', episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
